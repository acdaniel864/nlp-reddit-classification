{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Production Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from functions import gs_eval\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning); # stop warnings being printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>name</th>\n",
       "      <th>upvote ratio</th>\n",
       "      <th>num_upvotes</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>is_stoicism</th>\n",
       "      <th>word_count</th>\n",
       "      <th>contains_https</th>\n",
       "      <th>contains_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Looking for Seneca's quote on why even bed fle...</td>\n",
       "      <td>I think it was Seneca who wrote something alon...</td>\n",
       "      <td>Stoicism</td>\n",
       "      <td>1.705696e+09</td>\n",
       "      <td>t3_19aswwj</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "      <td>Looking for Senecas quote on why even bed flea...</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>READ BEFORE POSTING: r/Stoicism beginner's gui...</td>\n",
       "      <td>Welcome to the r/Stoicism subreddit, a forum f...</td>\n",
       "      <td>Stoicism</td>\n",
       "      <td>1.705694e+09</td>\n",
       "      <td>t3_19as7c7</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2</td>\n",
       "      <td>READ BEFORE POSTING rStoicism beginners guide ...</td>\n",
       "      <td>1</td>\n",
       "      <td>208</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New Agora: Daily WWYD and light discussion...</td>\n",
       "      <td>Welcome to the New Agora, a place for you and ...</td>\n",
       "      <td>Stoicism</td>\n",
       "      <td>1.705694e+09</td>\n",
       "      <td>t3_19as6qt</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2</td>\n",
       "      <td>The New Agora Daily WWYD and light discussion ...</td>\n",
       "      <td>1</td>\n",
       "      <td>237</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My biggest life mistake was wanting to live an...</td>\n",
       "      <td>2023 summons this the best, I didn’t want to e...</td>\n",
       "      <td>Stoicism</td>\n",
       "      <td>1.705691e+09</td>\n",
       "      <td>t3_19aqv6w</td>\n",
       "      <td>0.94</td>\n",
       "      <td>27</td>\n",
       "      <td>My biggest life mistake was wanting to live an...</td>\n",
       "      <td>1</td>\n",
       "      <td>380</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What’s your favorite way to practice gratitude...</td>\n",
       "      <td>You can mention some relevant quotes as well.</td>\n",
       "      <td>Stoicism</td>\n",
       "      <td>1.705691e+09</td>\n",
       "      <td>t3_19aqp1z</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>Whats your favorite way to practice gratitude ...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Looking for Seneca's quote on why even bed fle...   \n",
       "1  READ BEFORE POSTING: r/Stoicism beginner's gui...   \n",
       "2  The New Agora: Daily WWYD and light discussion...   \n",
       "3  My biggest life mistake was wanting to live an...   \n",
       "4  What’s your favorite way to practice gratitude...   \n",
       "\n",
       "                                            selftext subreddit   created_utc  \\\n",
       "0  I think it was Seneca who wrote something alon...  Stoicism  1.705696e+09   \n",
       "1  Welcome to the r/Stoicism subreddit, a forum f...  Stoicism  1.705694e+09   \n",
       "2  Welcome to the New Agora, a place for you and ...  Stoicism  1.705694e+09   \n",
       "3  2023 summons this the best, I didn’t want to e...  Stoicism  1.705691e+09   \n",
       "4      You can mention some relevant quotes as well.  Stoicism  1.705691e+09   \n",
       "\n",
       "         name  upvote ratio  num_upvotes  \\\n",
       "0  t3_19aswwj          0.67            1   \n",
       "1  t3_19as7c7          0.76            2   \n",
       "2  t3_19as6qt          0.76            2   \n",
       "3  t3_19aqv6w          0.94           27   \n",
       "4  t3_19aqp1z          1.00            3   \n",
       "\n",
       "                                       combined_text  is_stoicism  word_count  \\\n",
       "0  Looking for Senecas quote on why even bed flea...            1          64   \n",
       "1  READ BEFORE POSTING rStoicism beginners guide ...            1         208   \n",
       "2  The New Agora Daily WWYD and light discussion ...            1         237   \n",
       "3  My biggest life mistake was wanting to live an...            1         380   \n",
       "4  Whats your favorite way to practice gratitude ...            1          18   \n",
       "\n",
       "   contains_https  contains_emoji  \n",
       "0           False           False  \n",
       "1            True           False  \n",
       "2            True           False  \n",
       "3           False           False  \n",
       "4           False           False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/stoicism_buddhism_clean.csv', lineterminator='\\n')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_stoicism\n",
       "0    0.517519\n",
       "1    0.482481\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['is_stoicism']\n",
    "X = df['combined_text']\n",
    "\n",
    "y.value_counts(normalize=True) # find baseline \n",
    "# baseline is 51% of posts are in the stoicism subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a first simple LogisticRegression model with minimal manipulation, CountVectorizer() to process text\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cv: 0.9043065396395186\n"
     ]
    }
   ],
   "source": [
    "cvec = CountVectorizer()\n",
    "logreg = LogisticRegression(max_iter=10000, random_state=42)\n",
    "X_train_cvec = cvec.fit_transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)\n",
    "print(f\"Train cv: {cross_val_score(logreg, X_train_cvec, y_train, cv = 5).mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary logistic regression score:**<br/>\n",
    "Train cv: 0.9043065396395186 <br/>\n",
    "\n",
    "Using this for the purposes of comparision to other models and to compare benefits (or otherwise) of hyper parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24562.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.010994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.656653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.043217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.995066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.001509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>82.918133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Coefficient\n",
       "count  24562.000000\n",
       "mean       1.010994\n",
       "std        0.656653\n",
       "min        0.043217\n",
       "25%        0.995066\n",
       "50%        0.999998\n",
       "75%        1.001509\n",
       "max       82.918133"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check coefficients \n",
    "logreg_coefficients = logreg.fit(X_train_cvec, y_train).coef_\n",
    "feature_coefficients = pd.DataFrame({'Feature': cvec.get_feature_names_out(), 'Coefficient': np.exp(logreg_coefficients[0])})\n",
    "feature_coefficients.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficient Analysis:** It appears a few words in particular are doing a lot for the fit of our model with max of 82 and a mean/median of ~1. View top coefficents below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20659</th>\n",
       "      <td>stoicism</td>\n",
       "      <td>82.918133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20649</th>\n",
       "      <td>stoic</td>\n",
       "      <td>55.413986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7507</th>\n",
       "      <td>epictetus</td>\n",
       "      <td>14.324322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20679</th>\n",
       "      <td>stoics</td>\n",
       "      <td>13.349337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19306</th>\n",
       "      <td>seneca</td>\n",
       "      <td>11.661075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13182</th>\n",
       "      <td>marcus</td>\n",
       "      <td>8.949635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13414</th>\n",
       "      <td>meditations</td>\n",
       "      <td>6.202072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>aurelius</td>\n",
       "      <td>5.998471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17351</th>\n",
       "      <td>quote</td>\n",
       "      <td>3.696446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19309</th>\n",
       "      <td>senecas</td>\n",
       "      <td>3.604657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>control</td>\n",
       "      <td>3.497128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>mori</td>\n",
       "      <td>3.397108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16080</th>\n",
       "      <td>philosophy</td>\n",
       "      <td>3.068774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13092</th>\n",
       "      <td>man</td>\n",
       "      <td>2.792398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>belongs</td>\n",
       "      <td>2.773125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13460</th>\n",
       "      <td>memento</td>\n",
       "      <td>2.626612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10164</th>\n",
       "      <td>hit</td>\n",
       "      <td>2.534321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11893</th>\n",
       "      <td>jordan</td>\n",
       "      <td>2.529776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15046</th>\n",
       "      <td>often</td>\n",
       "      <td>2.455601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17951</th>\n",
       "      <td>relevant</td>\n",
       "      <td>2.362874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15173</th>\n",
       "      <td>opinions</td>\n",
       "      <td>2.362511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12738</th>\n",
       "      <td>living</td>\n",
       "      <td>2.341685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24001</th>\n",
       "      <td>wisdom</td>\n",
       "      <td>2.308847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22230</th>\n",
       "      <td>training</td>\n",
       "      <td>2.239572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12826</th>\n",
       "      <td>look</td>\n",
       "      <td>2.238142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16024</th>\n",
       "      <td>peterson</td>\n",
       "      <td>2.202256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12728</th>\n",
       "      <td>live</td>\n",
       "      <td>2.178526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23498</th>\n",
       "      <td>virtue</td>\n",
       "      <td>2.173775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12511</th>\n",
       "      <td>less</td>\n",
       "      <td>2.151824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22100</th>\n",
       "      <td>tomorrow</td>\n",
       "      <td>2.128894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8241</th>\n",
       "      <td>fate</td>\n",
       "      <td>2.122660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11146</th>\n",
       "      <td>indifference</td>\n",
       "      <td>2.100672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>cannot</td>\n",
       "      <td>2.089351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15379</th>\n",
       "      <td>over</td>\n",
       "      <td>2.085006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3573</th>\n",
       "      <td>care</td>\n",
       "      <td>2.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18202</th>\n",
       "      <td>resolution</td>\n",
       "      <td>2.015595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16576</th>\n",
       "      <td>practices</td>\n",
       "      <td>2.008371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>belief</td>\n",
       "      <td>2.003013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>belong</td>\n",
       "      <td>1.999857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21662</th>\n",
       "      <td>thankful</td>\n",
       "      <td>1.988833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>above</td>\n",
       "      <td>1.981340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22387</th>\n",
       "      <td>tries</td>\n",
       "      <td>1.969043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9714</th>\n",
       "      <td>handle</td>\n",
       "      <td>1.965817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9553</th>\n",
       "      <td>grumpy</td>\n",
       "      <td>1.958282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23839</th>\n",
       "      <td>went</td>\n",
       "      <td>1.930329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9334</th>\n",
       "      <td>gonna</td>\n",
       "      <td>1.909171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>artist</td>\n",
       "      <td>1.908622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16378</th>\n",
       "      <td>political</td>\n",
       "      <td>1.896765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24104</th>\n",
       "      <td>working</td>\n",
       "      <td>1.895308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10321</th>\n",
       "      <td>hour</td>\n",
       "      <td>1.893463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature  Coefficient\n",
       "20659      stoicism    82.918133\n",
       "20649         stoic    55.413986\n",
       "7507      epictetus    14.324322\n",
       "20679        stoics    13.349337\n",
       "19306        seneca    11.661075\n",
       "13182        marcus     8.949635\n",
       "13414   meditations     6.202072\n",
       "2109       aurelius     5.998471\n",
       "17351         quote     3.696446\n",
       "19309       senecas     3.604657\n",
       "4895        control     3.497128\n",
       "13998          mori     3.397108\n",
       "16080    philosophy     3.068774\n",
       "13092           man     2.792398\n",
       "2576        belongs     2.773125\n",
       "13460       memento     2.626612\n",
       "10164           hit     2.534321\n",
       "11893        jordan     2.529776\n",
       "15046         often     2.455601\n",
       "17951      relevant     2.362874\n",
       "15173      opinions     2.362511\n",
       "12738        living     2.341685\n",
       "24001        wisdom     2.308847\n",
       "22230      training     2.239572\n",
       "12826          look     2.238142\n",
       "16024      peterson     2.202256\n",
       "12728          live     2.178526\n",
       "23498        virtue     2.173775\n",
       "12511          less     2.151824\n",
       "22100      tomorrow     2.128894\n",
       "8241           fate     2.122660\n",
       "11146  indifference     2.100672\n",
       "3525         cannot     2.089351\n",
       "15379          over     2.085006\n",
       "3573           care     2.037543\n",
       "18202    resolution     2.015595\n",
       "16576     practices     2.008371\n",
       "2546         belief     2.003013\n",
       "2571         belong     1.999857\n",
       "21662      thankful     1.988833\n",
       "620           above     1.981340\n",
       "22387         tries     1.969043\n",
       "9714         handle     1.965817\n",
       "9553         grumpy     1.958282\n",
       "23839          went     1.930329\n",
       "9334          gonna     1.909171\n",
       "1852         artist     1.908622\n",
       "16378     political     1.896765\n",
       "24104       working     1.895308\n",
       "10321          hour     1.893463"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list largest odds coefficients\n",
    "feature_coefficients.sort_values('Coefficient', ascending=False).head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many stoicism related words in the largest coefficients. Stoicism, stoic, seneca, epictetus, stoics, marcus, aurelius, momento, mori. It is to be expected that these words being present in a reddit submission - all else being equal - would increase the chances of the text being in r/stoicism the most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21428</th>\n",
       "      <td>tara</td>\n",
       "      <td>0.510130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>tibetan</td>\n",
       "      <td>0.506690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14593</th>\n",
       "      <td>nirvana</td>\n",
       "      <td>0.506174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10926</th>\n",
       "      <td>impermanent</td>\n",
       "      <td>0.505722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22356</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.503715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18826</th>\n",
       "      <td>samsara</td>\n",
       "      <td>0.499726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17647</th>\n",
       "      <td>recently</td>\n",
       "      <td>0.494960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21204</th>\n",
       "      <td>sutra</td>\n",
       "      <td>0.491212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13946</th>\n",
       "      <td>monks</td>\n",
       "      <td>0.490395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22465</th>\n",
       "      <td>try</td>\n",
       "      <td>0.482939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20553</th>\n",
       "      <td>statue</td>\n",
       "      <td>0.479225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15535</th>\n",
       "      <td>painting</td>\n",
       "      <td>0.477887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>hanh</td>\n",
       "      <td>0.476731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13066</th>\n",
       "      <td>mala</td>\n",
       "      <td>0.474054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18852</th>\n",
       "      <td>sangha</td>\n",
       "      <td>0.473505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17621</th>\n",
       "      <td>rebirth</td>\n",
       "      <td>0.461292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10925</th>\n",
       "      <td>impermanence</td>\n",
       "      <td>0.460825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17893</th>\n",
       "      <td>reincarnation</td>\n",
       "      <td>0.458379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>beings</td>\n",
       "      <td>0.447018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10642</th>\n",
       "      <td>hurt</td>\n",
       "      <td>0.439709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14535</th>\n",
       "      <td>nhat</td>\n",
       "      <td>0.429652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17508</th>\n",
       "      <td>rbuddhism</td>\n",
       "      <td>0.429097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9559</th>\n",
       "      <td>guanyin</td>\n",
       "      <td>0.406279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>peace</td>\n",
       "      <td>0.402607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21780</th>\n",
       "      <td>thich</td>\n",
       "      <td>0.397715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15797</th>\n",
       "      <td>peaceful</td>\n",
       "      <td>0.393946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24389</th>\n",
       "      <td>zen</td>\n",
       "      <td>0.388921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>bodhisattva</td>\n",
       "      <td>0.385039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12530</th>\n",
       "      <td>letting</td>\n",
       "      <td>0.373130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18455</th>\n",
       "      <td>rinpoche</td>\n",
       "      <td>0.367124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13941</th>\n",
       "      <td>monk</td>\n",
       "      <td>0.357195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7404</th>\n",
       "      <td>enlightenment</td>\n",
       "      <td>0.348405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19653</th>\n",
       "      <td>shrine</td>\n",
       "      <td>0.346851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16572</th>\n",
       "      <td>practice</td>\n",
       "      <td>0.343268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>0.330480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>dharma</td>\n",
       "      <td>0.328640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5452</th>\n",
       "      <td>dalai</td>\n",
       "      <td>0.308984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>buddhas</td>\n",
       "      <td>0.305688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>altar</td>\n",
       "      <td>0.304817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4471</th>\n",
       "      <td>compassion</td>\n",
       "      <td>0.301858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13916</th>\n",
       "      <td>monastery</td>\n",
       "      <td>0.286683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13411</th>\n",
       "      <td>meditation</td>\n",
       "      <td>0.278213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>ajahn</td>\n",
       "      <td>0.268435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>buddhists</td>\n",
       "      <td>0.264887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12295</th>\n",
       "      <td>lama</td>\n",
       "      <td>0.234448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21559</th>\n",
       "      <td>temple</td>\n",
       "      <td>0.219011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12043</th>\n",
       "      <td>karma</td>\n",
       "      <td>0.148231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>buddhism</td>\n",
       "      <td>0.046370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>buddha</td>\n",
       "      <td>0.043883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>buddhist</td>\n",
       "      <td>0.043217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Coefficient\n",
       "21428           tara     0.510130\n",
       "21944        tibetan     0.506690\n",
       "14593        nirvana     0.506174\n",
       "10926    impermanent     0.505722\n",
       "22356           tree     0.503715\n",
       "18826        samsara     0.499726\n",
       "17647       recently     0.494960\n",
       "21204          sutra     0.491212\n",
       "13946          monks     0.490395\n",
       "22465            try     0.482939\n",
       "20553         statue     0.479225\n",
       "15535       painting     0.477887\n",
       "9737            hanh     0.476731\n",
       "13066           mala     0.474054\n",
       "18852         sangha     0.473505\n",
       "17621        rebirth     0.461292\n",
       "10925   impermanence     0.460825\n",
       "17893  reincarnation     0.458379\n",
       "2535          beings     0.447018\n",
       "10642           hurt     0.439709\n",
       "14535           nhat     0.429652\n",
       "17508      rbuddhism     0.429097\n",
       "9559         guanyin     0.406279\n",
       "15795          peace     0.402607\n",
       "21780          thich     0.397715\n",
       "15797       peaceful     0.393946\n",
       "24389            zen     0.388921\n",
       "2897     bodhisattva     0.385039\n",
       "12530        letting     0.373130\n",
       "18455       rinpoche     0.367124\n",
       "13941           monk     0.357195\n",
       "7404   enlightenment     0.348405\n",
       "19653         shrine     0.346851\n",
       "16572       practice     0.343268\n",
       "2440       beautiful     0.330480\n",
       "6108          dharma     0.328640\n",
       "5452           dalai     0.308984\n",
       "3246         buddhas     0.305688\n",
       "1254           altar     0.304817\n",
       "4471      compassion     0.301858\n",
       "13916      monastery     0.286683\n",
       "13411     meditation     0.278213\n",
       "1126           ajahn     0.268435\n",
       "3282       buddhists     0.264887\n",
       "12295           lama     0.234448\n",
       "21559         temple     0.219011\n",
       "12043          karma     0.148231\n",
       "3257        buddhism     0.046370\n",
       "3225          buddha     0.043883\n",
       "3273        buddhist     0.043217"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list smallest coefficents \n",
    "feature_coefficients.sort_values('Coefficient', ascending=False).tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely we see many Buddhism related words having the lowest coefficients in the model. Meaning these words being present in a reddit submission - all else being held equal - increase the odds of the post being in the stoicism subreddit the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the       21576\n",
       "to        19759\n",
       "and       17477\n",
       "of        13577\n",
       "is         9670\n",
       "in         9427\n",
       "that       8700\n",
       "it         7305\n",
       "you        7000\n",
       "my         6332\n",
       "this       5859\n",
       "for        5409\n",
       "but        4535\n",
       "not        4478\n",
       "have       4393\n",
       "be         4358\n",
       "with       4275\n",
       "me         4050\n",
       "as         3999\n",
       "are        3975\n",
       "or         3878\n",
       "on         3809\n",
       "what       3311\n",
       "if         3227\n",
       "was        3015\n",
       "do         2989\n",
       "so         2758\n",
       "about      2753\n",
       "we         2647\n",
       "how        2639\n",
       "can        2570\n",
       "im         2547\n",
       "from       2461\n",
       "like       2457\n",
       "all        2369\n",
       "your       2343\n",
       "they       2147\n",
       "just       2108\n",
       "life       2059\n",
       "by         1954\n",
       "at         1880\n",
       "its        1868\n",
       "one        1859\n",
       "people     1849\n",
       "dont       1838\n",
       "am         1804\n",
       "would      1786\n",
       "he         1779\n",
       "when       1758\n",
       "will       1710\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list top ocurring words overall\n",
    "X_train_df = pd.DataFrame(X_train_cvec.todense(), columns=cvec.get_feature_names_out())\n",
    "X_train_df.sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings from initial model:** \n",
    "- Our initial model had quite a high accuracy at 0.9 cross validation. \n",
    "- The results of our coefficent analysis suggest that stemming might: 1. Help the model generalise better if we encounter overfitting and: 2. Improve model performance/reduce computational costs. \n",
    "- Even in the top/bottom 25 coefficients, it can be observed that many of the words can be grouped together by their stems e.g. buddhist, buddha, buddhism, buddhists, buddhas.\n",
    "- Try including stop words. Given so many of the top occuring words are very common non-distinct words, including stop words will be explored. I will make a list of custom stop words, removing typical high occuring stop words (pronouns, prepositions, conjunctions) and compare custom stop words with None and 'english' standardised list to see how it affects our model performance. \n",
    "- It will be interesting, later in the project to try removing philosophers names and other highly specific terminology to see how it effects model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cv: 0.9052939011781748\n"
     ]
    }
   ],
   "source": [
    "# source: https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn\n",
    "\n",
    "def stemmed_words(doc):\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return (stemmer.stem(word) for word in word_tokenize(doc)) \n",
    "\n",
    "cvec_stem = CountVectorizer(tokenizer=stemmed_words, max_features=5000)\n",
    "logreg = LogisticRegression(max_iter=10000, random_state=42)\n",
    "X_train_stem = cvec_stem.fit_transform(X_train)\n",
    "\n",
    "print(f\"Train cv: {cross_val_score(logreg, X_train_stem, y_train, cv = 5).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the     21576\n",
       "to      19759\n",
       "i       19555\n",
       "and     17478\n",
       "of      13577\n",
       "a       13312\n",
       "is       9671\n",
       "in       9429\n",
       "it       9173\n",
       "that     9068\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view most commmon stemmed words\n",
    "pd.DataFrame(X_train_stem .todense(), columns=cvec_stem.get_feature_names_out()).sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>stoicism</td>\n",
       "      <td>4.600606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>stoic</td>\n",
       "      <td>4.318721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>epictetus</td>\n",
       "      <td>2.741697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>seneca</td>\n",
       "      <td>2.695899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>marcus</td>\n",
       "      <td>2.360427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>aurelius</td>\n",
       "      <td>1.934431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>belong</td>\n",
       "      <td>1.479674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>man</td>\n",
       "      <td>1.293385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>control</td>\n",
       "      <td>1.281660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>quot</td>\n",
       "      <td>1.202710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature  Coefficient\n",
       "4174   stoicism     4.600606\n",
       "4172      stoic     4.318721\n",
       "1526  epictetus     2.741697\n",
       "3869     seneca     2.695899\n",
       "2712     marcus     2.360427\n",
       "407    aurelius     1.934431\n",
       "499      belong     1.479674\n",
       "2697        man     1.293385\n",
       "992     control     1.281660\n",
       "3484       quot     1.202710"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check largest coefficients \n",
    "logreg_coefficients_stem = logreg.fit(X_train_stem, y_train).coef_\n",
    "feature_coefficients_stem = pd.DataFrame({'Feature': cvec_stem.get_feature_names_out(), 'Coefficient': (logreg_coefficients_stem[0])})\n",
    "feature_coefficients_stem.sort_values('Coefficient', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>altar</td>\n",
       "      <td>-1.278735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>compass</td>\n",
       "      <td>-1.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>ajahn</td>\n",
       "      <td>-1.340418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>monasteri</td>\n",
       "      <td>-1.340427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>templ</td>\n",
       "      <td>-1.591266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>lama</td>\n",
       "      <td>-1.622890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>karma</td>\n",
       "      <td>-2.096996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>buddhism</td>\n",
       "      <td>-3.225170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>buddhist</td>\n",
       "      <td>-3.252513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>buddha</td>\n",
       "      <td>-3.287818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature  Coefficient\n",
       "227       altar    -1.278735\n",
       "904     compass    -1.315900\n",
       "199       ajahn    -1.340418\n",
       "2846  monasteri    -1.340427\n",
       "4378      templ    -1.591266\n",
       "2524       lama    -1.622890\n",
       "2477      karma    -2.096996\n",
       "653    buddhism    -3.225170\n",
       "654    buddhist    -3.252513\n",
       "647      buddha    -3.287818"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check smallest coefficients \n",
    "feature_coefficients_stem.sort_values('Coefficient', ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming with Logistic Regression Summary:**\n",
    "- Slight improvement in model cross validation score, suggesting stemming might be helpful particularly if overfitting is  encountered. <br/>\n",
    "- However, stemming has not worked exactly as intended, many of the words remain unchanged in the largest and smallest coefficients and therefore have not been grouped see buddhism, buddhist, buddha. <br/>\n",
    "- Models coefficients for 'stoicism' and 'stoic' have increased. <br/>\n",
    "- Both the PorterStemmer and SnowballStemmer were tested. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Comparison with Logistic Regression: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stop words made by iteratively viewing the top appearing words. \n",
    "# A list of common words that will not contribute to explaining or interpretting the results \n",
    "\n",
    "custom_sw = ['about', 'all', 'also', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be',  'but', 'by', 'can', 'do', \n",
    "             'don', 'even', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'him', 'his', 'how', 'if', \n",
    "             'in', 'is', 'it', 'just', 'like', 'me', 'much', 'https', 'my', 'not', 'now', 'of', 'on', 'one', 'or', 'people', \n",
    "             'she', 'so', 'some', 'that', 'the', 'they', 'this', 'to', 've', 'was', 'we', 'what', \n",
    "             'when', 'which', 'who', 'will', 'with', 'would', 'you', 'your',  'their', 'other', \n",
    "             'something', 'want', 'only', 'then', 'really', 'day', 'own']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test stop words \n",
    "\n",
    "pipe_sw = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('logreg', LogisticRegression(max_iter=100000, random_state=42))\n",
    "])\n",
    "\n",
    "params_sw = {'cvec__stop_words': [None, 'english', custom_sw]\n",
    "          }\n",
    "\n",
    "lgrg_gs = GridSearchCV(pipe_sw,\n",
    "                  param_grid = params_sw, \n",
    "                  cv = 5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "lgrg_sw = lgrg_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'cvec__stop_words': 'english'}\n",
      "Best score: 0.9088445864510867\n",
      "Train score:  0.9903314917127072\n",
      "Test score:  0.9100946372239748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('cvec', CountVectorizer(stop_words='english')),\n",
       "                 ('logreg',\n",
       "                  LogisticRegression(max_iter=100000, random_state=42))]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pre-defined function for evaluating models after a grid search - see functions.py\n",
    "\n",
    "gs_eval(X_train, y_train, X_test, y_test, lgrg_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of stop-word testing with Logistic Regression:**\n",
    "- Best stop words in terms of model performance based on cross valitated score found were 'english', in comparison with custom list and None. \n",
    "- Model is highly overfit. \n",
    "- Below hyperparameter tuning is done with the aim of reducing test score bias, reducing variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Logistic Regression with Count Vectorizer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('logreg', LogisticRegression(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "params = {'cvec__ngram_range': [(1,2)],\n",
    "          'cvec__max_df' : [0.3, 0.5, 0.7],\n",
    "          'cvec__min_df' : [2],\n",
    "          'logreg__C' : [0.3, 0.6, 0.9]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'cvec__max_df': 0.3, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'logreg__C': 0.3}\n",
      "Best score: 0.9086475424997712\n",
      "Train score:  0.9757300710339384\n",
      "Test score:  0.9100946372239748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('cvec',\n",
       "                  CountVectorizer(max_df=0.3, min_df=2, ngram_range=(1, 2),\n",
       "                                  stop_words='english')),\n",
       "                 ('logreg',\n",
       "                  LogisticRegression(C=0.3, max_iter=10000, random_state=42))]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_gs = GridSearchCV(pipe,\n",
    "                  param_grid = params, \n",
    "                  cv = 5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "logreg_gs.fit(X_train, y_train)\n",
    "\n",
    "gs_eval(X_train, y_train, X_test, y_test,logreg_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results of hyperparameter tuning logreg with cvec v1:** <br/>\n",
    "Best params: {'cvec__max_df': 0.9, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'logreg__C': 1, 'logreg__penalty': 'l2'}<br/>\n",
    "Best score: 0.9171282051282053<br/>\n",
    "Train score:  0.992<br/>\n",
    "Test score:  0.911402789171452<br/>\n",
    "\n",
    "**Results of hyperparameter tuning logreg with cvec v2:**<br/>\n",
    "Best params: {'cvec__max_df': 0.87, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'logreg__C': 1}<br/>\n",
    "Best score: 0.9171282051282053<br/>\n",
    "Train score:  0.992<br/>\n",
    "Test score:  0.911402789171452<br/>\n",
    "\n",
    "**Results of hyperparameter tuning logreg with cvec v3:**<br/>\n",
    "Best params: {'cvec__max_df': 0.7, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'logreg__C': 0.95}<br/>\n",
    "Best score: 0.9173333333333333<br/>\n",
    "Train score:  0.9911794871794872<br/>\n",
    "Test score:  0.9105824446267432<br/>\n",
    "\n",
    "**Insights:**\n",
    "- Several iterations of hyperparameter tuning were completed - some listed above. Including tuning of 'logreg__penalty', stop words, 'logreg__C', 'min_df' and more. \n",
    "- An improvement on previous model accuracy is observed. <br/>\n",
    "- Attempted to improve overfitting with increaed regularization (lowering C) however bias increased without much improvement in varience. <br/>\n",
    "- The model is highly overfit: below we will attempt to use TF-DIF to reduce this overfitting and perhaps word stemming. <br/>\n",
    "\n",
    "<br/>\n",
    "*Note: gridsearch params have been limited from original gridsearches for computational speed purposes. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Logistic Regression with TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', min_df=1)),\n",
    "    ('logreg', LogisticRegression(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "params2 = {'tfidf__ngram_range': [(1,1)],\n",
    "          'tfidf__max_df' : [0.5, 0.7, 0.9],\n",
    "          'tfidf__max_features' : [3500, 4000, 4500],\n",
    "          'logreg__C' : [0.96, 0.97, 0.99]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'logreg__C': 0.99, 'tfidf__max_df': 0.5, 'tfidf__max_features': 4500, 'tfidf__ngram_range': (1, 1)}\n",
      "Best score: 0.9157493024605182\n",
      "Train score:  0.9640883977900553\n",
      "Test score:  0.9250788643533123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('tfidf',\n",
       "                  TfidfVectorizer(max_df=0.5, max_features=4500,\n",
       "                                  stop_words='english')),\n",
       "                 ('logreg',\n",
       "                  LogisticRegression(C=0.99, max_iter=10000, random_state=42))]),\n",
       " array([0, 0, 0, ..., 1, 0, 0]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_tfidf = GridSearchCV(pipe2,\n",
    "                  param_grid = params2, \n",
    "                  cv = 5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "logreg_tfidf.fit(X_train, y_train)\n",
    "\n",
    "gs_eval(X_train, y_train, X_test, y_test, logreg_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results LogisticRegression with TF-IDF v1:** <br/>\n",
    "Best parameters: {'logreg__C': 0.97, 'tfidf__max_df': 0.5, 'tfidf__max_features': 4000, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 1)}<br/>\n",
    "Best score: 0.9182958907636125<br/>\n",
    "Train score:  0.9618989405052975<br/>\n",
    "Test score:  0.9185667752442996<br/>\n",
    "\n",
    "**Results LogisticRegression with TF-IDF v2:**<br/>\n",
    "Best parameters: {'logreg__C': 0.99, 'tfidf__max_df': 0.5, 'tfidf__max_features': 4500, 'tfidf__ngram_range': (1, 1)}<br/>\n",
    "Best score: 0.9157493024605182<br/>\n",
    "Train score:  0.9640883977900553<br/>\n",
    "Test score:  0.9250788643533123<br/>\n",
    "\n",
    "**Insights:**<br/>\n",
    "- Reduced variance and lower bias compared to CVEC pre-processing. <br/>\n",
    "- Highest cross validation scores observed in v1. <br/>\n",
    "- Optimises with ngram range 1,1 unlike CVEC. <br/>\n",
    "- Lowest bias in test dataset with v2. <br/>\n",
    "\n",
    "<br/>\n",
    "*Note: gridsearch parameters inputted above have been limited from original gridsearches for computational speed. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TF-IDF - Scoring and Stemming: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'logreg__C': 0.99, 'tfidf__max_df': 0.3, 'tfidf__max_features': 3000, 'tfidf__ngram_range': (1, 1)}\n",
      "Best score: 0.9142485052841911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9571823204419889\n",
      "Test score:  0.9274447949526814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('tfidf',\n",
       "                  TfidfVectorizer(max_df=0.3, max_features=3000,\n",
       "                                  stop_words='english',\n",
       "                                  tokenizer=<function stemmed_words at 0x142e436a0>)),\n",
       "                 ('logreg',\n",
       "                  LogisticRegression(C=0.99, max_iter=10000, random_state=42))]),\n",
       " array([0, 0, 0, ..., 1, 0, 0]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2_1 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', tokenizer=stemmed_words, min_df=1)),\n",
    "    ('logreg', LogisticRegression(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "params2_1 = {'tfidf__ngram_range': [(1,1)],\n",
    "          'tfidf__max_df' : [0.3],\n",
    "          'tfidf__max_features' : [3000],\n",
    "          'logreg__C' : [0.99]\n",
    "          }\n",
    "\n",
    "logreg_tfidf_1 = GridSearchCV(pipe2_1,\n",
    "                  param_grid = params2_1, \n",
    "                  cv = 5,\n",
    "                  n_jobs=-1, \n",
    "                  scoring='balanced_accuracy')\n",
    "\n",
    "logreg_tfidf_1 = logreg_tfidf_1.fit(X_train, y_train)\n",
    "\n",
    "gs_eval(X_train, y_train, X_test, y_test, logreg_tfidf_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results LogisticRegression with TF-IDF v3 - scoring onbalanced_accuracy:**<br/>\n",
    "Best parameters: {'logreg__C': 0.99, 'tfidf__max_df': 0.3, 'tfidf__max_features': 4500, 'tfidf__ngram_range': (1, 1)}<br/>\n",
    "Best score: 0.9149147950661556<br/>\n",
    "Train score:  0.9640883977900553<br/>\n",
    "Test score:  0.9250788643533123<br/>\n",
    "\n",
    "**Results LogisticRegression with TF-IDF v4 - scoring on balanced_accuracy:**<br/>\n",
    "Best parameters: {'logreg__C': 0.3, 'tfidf__max_df': 0.3, 'tfidf__max_features': 4500, 'tfidf__ngram_range': (1, 1)}<br/>\n",
    "Best score: 0.9066780178094748<br/>\n",
    "Train score:  0.9374506708760852<br/>\n",
    "Test score:  0.9132492113564669<br/>\n",
    "\n",
    "**Results LogisticRegression with TF-IDF v5 - with stemming:**<br/>\n",
    "Best parameters: {'logreg__C': 0.99, 'tfidf__max_df': 0.3, 'tfidf__max_features': 3000, 'tfidf__ngram_range': (1, 1)}<br/>\n",
    "Best score: 0.9142485052841911<br/>\n",
    "Train score:  0.9571823204419889<br/>\n",
    "Test score:  0.9274447949526814<br/>\n",
    "\n",
    "<br/>\n",
    "*Note: gridsearch params have been limited from original gridsearches for computational speed purposes. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier with Count Vectoriser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "120 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "120 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py\", line 416, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py\", line 370, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_df' parameter of CountVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got None instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.70244377 0.70244377 0.70244377 0.70244377 0.84866343 0.8490583\n",
      " 0.84905869 0.84925631 0.88319227 0.88319227 0.88536287 0.88398103\n",
      " 0.67621064 0.67621064 0.67621064 0.67621064 0.84372915 0.84412383\n",
      " 0.84293942 0.84313685 0.87944318 0.87924613 0.87865422 0.87825896\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;cvec&#x27;,\n",
       "                                        CountVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                                       (&#x27;rf&#x27;,\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;cvec__max_df&#x27;: [0.99, None], &#x27;cvec__min_df&#x27;: [12, 15],\n",
       "                         &#x27;rf__max_depth&#x27;: [1, 5, 10],\n",
       "                         &#x27;rf__min_samples_leaf&#x27;: [1, 2],\n",
       "                         &#x27;rf__min_samples_split&#x27;: [10, 12]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;cvec&#x27;,\n",
       "                                        CountVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                                       (&#x27;rf&#x27;,\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;cvec__max_df&#x27;: [0.99, None], &#x27;cvec__min_df&#x27;: [12, 15],\n",
       "                         &#x27;rf__max_depth&#x27;: [1, 5, 10],\n",
       "                         &#x27;rf__min_samples_leaf&#x27;: [1, 2],\n",
       "                         &#x27;rf__min_samples_split&#x27;: [10, 12]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;cvec&#x27;, CountVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;rf&#x27;, RandomForestClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec',\n",
       "                                        CountVectorizer(stop_words='english')),\n",
       "                                       ('rf',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.99, None], 'cvec__min_df': [12, 15],\n",
       "                         'rf__max_depth': [1, 5, 10],\n",
       "                         'rf__min_samples_leaf': [1, 2],\n",
       "                         'rf__min_samples_split': [10, 12]})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rf = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "params_rf = {\n",
    "            'cvec__max_df': [0.99, None],\n",
    "            'cvec__min_df': [12, 15],\n",
    "            'rf__max_depth' : [1, 5, 10],\n",
    "            'rf__min_samples_leaf' : [1, 2],\n",
    "            'rf__min_samples_split' : [10, 12]\n",
    "            }\n",
    "\n",
    "rf_gs = GridSearchCV(pipe_rf,\n",
    "                  param_grid = params_rf, \n",
    "                  cv = 5,\n",
    "                  n_jobs=-1)\n",
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'cvec__max_df': 0.99, 'cvec__min_df': 12, 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 10}\n",
      "Best score: 0.8853628665611352\n",
      "Train score:  0.8942383583267561\n",
      "Test score:  0.8730283911671924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('cvec',\n",
       "                  CountVectorizer(max_df=0.99, min_df=12, stop_words='english')),\n",
       "                 ('rf',\n",
       "                  RandomForestClassifier(max_depth=10, min_samples_leaf=2,\n",
       "                                         min_samples_split=10,\n",
       "                                         random_state=42))]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_eval(X_train, y_train, X_test, y_test, rf_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                                       (&#x27;rf&#x27;,\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;rf__max_depth&#x27;: [10, 20],\n",
       "                         &#x27;rf__min_samples_leaf&#x27;: [2, 5],\n",
       "                         &#x27;rf__min_samples_split&#x27;: [5, 10],\n",
       "                         &#x27;rf__n_estimators&#x27;: [100, 200],\n",
       "                         &#x27;tfidf__max_df&#x27;: [0.3, 0.7],\n",
       "                         &#x27;tfidf__max_features&#x27;: [1000, 2000]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                                       (&#x27;rf&#x27;,\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;rf__max_depth&#x27;: [10, 20],\n",
       "                         &#x27;rf__min_samples_leaf&#x27;: [2, 5],\n",
       "                         &#x27;rf__min_samples_split&#x27;: [5, 10],\n",
       "                         &#x27;rf__n_estimators&#x27;: [100, 200],\n",
       "                         &#x27;tfidf__max_df&#x27;: [0.3, 0.7],\n",
       "                         &#x27;tfidf__max_features&#x27;: [1000, 2000]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;rf&#x27;, RandomForestClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words='english')),\n",
       "                                       ('rf',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': [10, 20],\n",
       "                         'rf__min_samples_leaf': [2, 5],\n",
       "                         'rf__min_samples_split': [5, 10],\n",
       "                         'rf__n_estimators': [100, 200],\n",
       "                         'tfidf__max_df': [0.3, 0.7],\n",
       "                         'tfidf__max_features': [1000, 2000]})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rf2 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,1))),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "params_rf2 = {\n",
    "            'tfidf__max_df' : [0.3, 0.7],\n",
    "            'tfidf__max_features' : [1000, 2000],\n",
    "            'rf__n_estimators' : [100, 200],\n",
    "            'rf__max_depth' : [10, 20], \n",
    "            'rf__min_samples_leaf' : [2, 5],\n",
    "            'rf__min_samples_split' : [5, 10]\n",
    "            }\n",
    "\n",
    "rf_gs2 = GridSearchCV(pipe_rf2,\n",
    "                  param_grid = params_rf2, \n",
    "                  cv = 5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "rf_gs2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'rf__max_depth': 20, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 10, 'rf__n_estimators': 200, 'tfidf__max_df': 0.3, 'tfidf__max_features': 1000}\n",
      "Best score: 0.900359429974435\n",
      "Train score:  0.9287687450670876\n",
      "Test score:  0.8919558359621451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('tfidf',\n",
       "                  TfidfVectorizer(max_df=0.3, max_features=1000,\n",
       "                                  stop_words='english')),\n",
       "                 ('rf',\n",
       "                  RandomForestClassifier(max_depth=20, min_samples_leaf=2,\n",
       "                                         min_samples_split=10, n_estimators=200,\n",
       "                                         random_state=42))]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_eval(X_train, y_train, X_test, y_test, rf_gs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results RandomForests with CVEC v1:** <br/>\n",
    "Best params: {'cvec__max_df': 0.99, 'cvec__min_df': 12, 'rf__max_depth': 12, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 12}<br/>\n",
    "Best score: 0.8879357486749253<br/>\n",
    "Train score:  0.9013854930725347<br/>\n",
    "Test score:  0.9013854930725347<br/>\n",
    "\n",
    "**Results RandomForests with TF-IDF v1:**<br/>\n",
    "Best parameters: {'rf__min_samples_split': 15, 'tfidf__max_df': 0.7, 'tfidf__max_features': 4500, 'tfidf__ngram_range': (1, 1)}<br/>\n",
    "Best score: 0.913608251275248<br/>\n",
    "Train score:  0.9969437652811736<br/>\n",
    "Test score:  0.9047231270358306<br/>\n",
    "\n",
    "**Results RandomForests with TF-IDF v2:**<br/>\n",
    "Best parameters: {'rf__min_samples_split': 15, 'tfidf__max_df': 0.7, 'tfidf__max_features': 4500, 'tfidf__ngram_range': (1, 1)}<br/>\n",
    "Best score: 0.913608251275248<br/>\n",
    "Train score:  0.9969437652811736<br/>\n",
    "Test score:  0.9047231270358306<br/>\n",
    "\n",
    "**Results RandomForests with TF-IDF v3:**<br/>\n",
    "Best parameters: {'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 15, 'rf__n_estimators': 200, 'tfidf__max_df': 0.7, 'tfidf__max_features': 1000}<br/>\n",
    "Best score: 0.8905840293478328<br/>\n",
    "Train score:  0.9019967400162999<br/>\n",
    "Test score:  0.8713355048859935<br/>\n",
    "\n",
    "**Results RandomForests with TF-IDF v4:**<br/>\n",
    "Best parameters: {'rf__max_depth': 20, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 10, 'rf__n_estimators': 100, 'tfidf__max_df': 0.3, 'tfidf__max_features': 1000}<br/>\n",
    "Best score: 0.8993449885917981<br/>\n",
    "Train score:  0.928280358598207<br/>\n",
    "Test score:  0.8908794788273615<br/>\n",
    "\n",
    "**Insights:**<br/>\n",
    "- All models performed worse with stop_words=None compared to stop_words = 'english'.<br/>\n",
    "- Iterated over different hyperparameter settings, including ngram range and several times to reduce varience without a large increase in bias.<br/>\n",
    "- RandomForests with CVEC performed worse than RandomForests with TF-IDF, both interms of varience and bias. <br/>\n",
    "- In attempting to reduce varience with hyper parameter tuning, bias increased disproportionality.<br/>\n",
    "- None of the Random Forests models so far outperform the best Logistic Regression model in regards to test data set accuracy or cross validated score. <br/>\n",
    "\n",
    "<br/>\n",
    "*Note: gridsearch params have been limited from original gridsearches for computational speed purposes. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests and TFIDF With Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=&#x27;english&#x27;,\n",
       "                                                        tokenizer=&lt;function stemmed_words at 0x142e436a0&gt;)),\n",
       "                                       (&#x27;rf&#x27;,\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;rf__max_depth&#x27;: [10, 20], &#x27;rf__min_samples_leaf&#x27;: [2],\n",
       "                         &#x27;rf__min_samples_split&#x27;: [5], &#x27;tfidf__max_df&#x27;: [0.9],\n",
       "                         &#x27;tfidf__max_features&#x27;: [2000]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=&#x27;english&#x27;,\n",
       "                                                        tokenizer=&lt;function stemmed_words at 0x142e436a0&gt;)),\n",
       "                                       (&#x27;rf&#x27;,\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;rf__max_depth&#x27;: [10, 20], &#x27;rf__min_samples_leaf&#x27;: [2],\n",
       "                         &#x27;rf__min_samples_split&#x27;: [5], &#x27;tfidf__max_df&#x27;: [0.9],\n",
       "                         &#x27;tfidf__max_features&#x27;: [2000]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(stop_words=&#x27;english&#x27;,\n",
       "                                 tokenizer=&lt;function stemmed_words at 0x142e436a0&gt;)),\n",
       "                (&#x27;rf&#x27;, RandomForestClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;,\n",
       "                tokenizer=&lt;function stemmed_words at 0x142e436a0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words='english',\n",
       "                                                        tokenizer=<function stemmed_words at 0x142e436a0>)),\n",
       "                                       ('rf',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': [10, 20], 'rf__min_samples_leaf': [2],\n",
       "                         'rf__min_samples_split': [5], 'tfidf__max_df': [0.9],\n",
       "                         'tfidf__max_features': [2000]})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rf3 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,1), tokenizer=stemmed_words)),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "params_rf3 = {\n",
    "            'tfidf__max_df' : [0.9],\n",
    "            'tfidf__max_features' : [2000],\n",
    "            'rf__max_depth' : [10, 20], \n",
    "            'rf__min_samples_leaf' : [2],\n",
    "            'rf__min_samples_split' : [5]\n",
    "            }\n",
    "\n",
    "rf_gs3 = GridSearchCV(pipe_rf3,\n",
    "                  param_grid = params_rf3, \n",
    "                  cv = 5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "rf_gs3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'rf__max_depth': 20, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'tfidf__max_df': 0.9, 'tfidf__max_features': 2000}\n",
      "Best score: 0.898385875141893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AaranDaniel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9352801894238358\n",
      "Test score:  0.9029968454258676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('tfidf',\n",
       "                  TfidfVectorizer(max_df=0.9, max_features=2000,\n",
       "                                  stop_words='english',\n",
       "                                  tokenizer=<function stemmed_words at 0x142e436a0>)),\n",
       "                 ('rf',\n",
       "                  RandomForestClassifier(max_depth=20, min_samples_leaf=2,\n",
       "                                         min_samples_split=5,\n",
       "                                         random_state=42))]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_eval(X_train, y_train, X_test, y_test, rf_gs3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results RandomForests with TF-IDF v5 - with stemming:**<br/>\n",
    "Best parameters: {'rf__max_depth': 20, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'tfidf__max_df': 0.7, 'tfidf__max_features': 2000}<br/>\n",
    "Best score: 0.9009525089029987<br/>\n",
    "Train score:  0.9356748224151539<br/>\n",
    "Test score:  0.8998422712933754<br/>\n",
    "\n",
    "- Reduced variance observed when stemming used. <br/>\n",
    "\n",
    "<br/>\n",
    "*Note: gridsearch params have been limited from original gridsearches for computational speed purposes. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests and TF-IDF - Hyperparameter Tuning Scoring = 'balanced accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                                       (&#x27;rf&#x27;,\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;rf__max_depth&#x27;: [30], &#x27;rf__min_samples_leaf&#x27;: [2],\n",
       "                         &#x27;rf__min_samples_split&#x27;: [15],\n",
       "                         &#x27;rf__n_estimators&#x27;: [600], &#x27;tfidf__max_df&#x27;: [0.1, 0.3],\n",
       "                         &#x27;tfidf__max_features&#x27;: [2000]},\n",
       "             scoring=&#x27;balanced_accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                                       (&#x27;rf&#x27;,\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;rf__max_depth&#x27;: [30], &#x27;rf__min_samples_leaf&#x27;: [2],\n",
       "                         &#x27;rf__min_samples_split&#x27;: [15],\n",
       "                         &#x27;rf__n_estimators&#x27;: [600], &#x27;tfidf__max_df&#x27;: [0.1, 0.3],\n",
       "                         &#x27;tfidf__max_features&#x27;: [2000]},\n",
       "             scoring=&#x27;balanced_accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;rf&#x27;, RandomForestClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words='english')),\n",
       "                                       ('rf',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': [30], 'rf__min_samples_leaf': [2],\n",
       "                         'rf__min_samples_split': [15],\n",
       "                         'rf__n_estimators': [600], 'tfidf__max_df': [0.1, 0.3],\n",
       "                         'tfidf__max_features': [2000]},\n",
       "             scoring='balanced_accuracy')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rf4 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,1))),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "params_rf4 = {\n",
    "            'tfidf__max_df' : [0.1, 0.3],\n",
    "            'tfidf__max_features' : [2000],\n",
    "            'rf__n_estimators' : [600],\n",
    "            'rf__max_depth' : [30], \n",
    "            'rf__min_samples_leaf' : [2],\n",
    "            'rf__min_samples_split' : [15]\n",
    "            }\n",
    "\n",
    "rf_gs4 = GridSearchCV(pipe_rf4,\n",
    "                  param_grid = params_rf4, \n",
    "                  cv = 5,\n",
    "                  n_jobs=-1, \n",
    "                  scoring='balanced_accuracy')\n",
    "\n",
    "rf_gs4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'rf__max_depth': 30, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 15, 'rf__n_estimators': 600, 'tfidf__max_df': 0.3, 'tfidf__max_features': 2000}\n",
      "Best score: 0.9021214683784816\n",
      "Train score:  0.9400157853196527\n",
      "Test score:  0.8990536277602523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('tfidf',\n",
       "                  TfidfVectorizer(max_df=0.3, max_features=2000,\n",
       "                                  stop_words='english')),\n",
       "                 ('rf',\n",
       "                  RandomForestClassifier(max_depth=30, min_samples_leaf=2,\n",
       "                                         min_samples_split=15, n_estimators=600,\n",
       "                                         random_state=42))]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_eval(X_train, y_train, X_test, y_test, rf_gs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results RandomForests with TF-IDF scoring based on 'balanced accuracy' v1:**<br/>\n",
    "Best parameters: {'rf__max_depth': 30, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 15, 'rf__n_estimators': 200, 'tfidf__max_df': 0.3, 'tfidf__max_features': 2000}<br/>\n",
    "Best score: 0.9020542892378233<br/>\n",
    "Train score:  0.9417916337805841<br/>\n",
    "Test score:  0.9006309148264984<br/>\n",
    "<br/>\n",
    "\n",
    "**Results RandomForests with TF-IDF scoring based on 'balanced accuracy' v2:**<br/>\n",
    "Best parameters: {'rf__max_depth': 30, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 15, 'rf__n_estimators': 600, 'tfidf__max_df': 0.3, 'tfidf__max_features': 2000}<br/>\n",
    "Best score: 0.9004197514345034<br/>\n",
    "Train score:  0.9408050513022889<br/>\n",
    "Test score:  0.9014195583596214<br/>\n",
    "\n",
    "\n",
    "*Note: gridsearch params have been limited from original gridsearches for computational speed purposes. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Brief Summary\n",
    "\n",
    "- Combination of CVEC pre-processing and TF-IFD were tested: TF-IDF produced lowest bias and lowest variance results. <br/>\n",
    "- Effects of stemming was explored and did not significantly improve model cross-validated score, though variance decreased slightly (improved generalisation). <br/>\n",
    "- Including default english stop words reduced bias and variance. <br/>\n",
    "- Two models focused on were: Logistic Regression and Random Forests. <br/>\n",
    "- Below selected best performing models based on Cross Validation score. <br/>\n",
    "- In the following workbook these are evaluated in more detail.<br/>\n",
    "- The production model Logistic Regression was chosen over Random Forests because of its better generalisation, lower bias on unseen data and higher cross validation accuracy. As well as interpretability of variables which will be explored further in the following workbook.<br/>\n",
    "- Of the many Logistic Regression models tested the model with TF-IDF v1 had the highest cross validation accuracy. <br/>\n",
    "\n",
    "**Looking Ahead:**\n",
    "- In the following workbook models are evaluated and compared in more detail using balanced accuracy, F1 scores, recall, precision and errors are analysed. \n",
    "- Visualisation of models are created and compared in the hope to find systematic differences between the two and improve understanding of the models. \n",
    "- Models with restricted vocabulary are considered, in the hope to reveal differences between the two subreddits and go some way towards answering the problem statement.\n",
    "\n",
    "### Production Model:\n",
    "**LogisticRegression model, with TF-IDF v1:** <br/>\n",
    "Best parameters: {'logreg__C': 0.97, 'tfidf__max_df': 0.5, 'tfidf__max_features': 4000, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 1)}<br/>\n",
    "Best score: 0.9182958907636125<br/>\n",
    "Train score:  0.9618989405052975<br/>\n",
    "Test score:  0.9185667752442996<br/>\n",
    "\n",
    "#### Other:\n",
    "**Best performing RandomForests model with TF-IDF v1:**<br/>\n",
    "Best parameters: {'rf__min_samples_split': 15, 'tfidf__max_df': 0.7, 'tfidf__max_features': 4500, 'tfidf__ngram_range': (1, 1)}<br/>\n",
    "Best score: 0.913608251275248<br/>\n",
    "Train score:  0.9969437652811736<br/>\n",
    "Test score:  0.9047231270358306<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
